\chapter{Interaction}
%TODO explain the interaction concept again
This chapter describes the various interaction levels with the bracelet in detail and illustrates the respective algorithms.

\section{Pairing the Bracelet with a Light Source}
The bracelet's Bluetooth device automatically searches for nearby devices and connects to a range of matching IDs without any interaction or confirmation by the user. This can pose a security risk to spoofing a lamp device, but since no sensitive data is handled by either participant, the impact would rather be an annoying disturbance than a serious threat to privacy.

\section{Switching the Light Source on and off}
Since turning the lights in a specific room on or off is a frequently used interaction, it should require few complexity in terms of cognitive as well as physical workload, i.e. a simple, easily memorable way of interaction is much desired. For those reasons, simply covering the touch surface with the whole hand and holding for a few seconds will result in switching the lights on or off, dependent on the current state.

\section{Adjusting the Light Source's Brightness}
Apart from switching the lights on or off, a change in brightness is the second most desired interaction in the smart lighting scenario. For example, the incentive of watching TV in the living room benefits from a dimmed light setting. However, if the user fails to locate the remote control for the television, a quick dim interaction towards brighter light facilitates the search for the missing remote control. After the item is found, the brightness level of the room's lighting can easily be dimmed back to the desired setting.

The dimming interaction is different from the other use cases, since there exists a physical solution for this task in form of dim knobs for wall outlets. Usually, those wall dimmers are rotary knobs connected to a potentiometer which dims the light source by increasing the electric resistance. Hence, the interaction of turning a knob for dimming the light level is an association for many people.

The intention was to preserve this association to make the interaction with the bracelet more intuitively. The dimming interaction is a combination of twisting the wrist while covering the bracelet's touch surface, imitating the interaction with the wall dimmer. A counterclockwise movement reduces the brightness, while a clockwise twist increases the brightness.

\section{Changing the Lighting Mood}
In addition to the brightness, users should be able to change the \textit{mood} of the current 

\section{Precise Touch Input for Color Change}

\section{Template picking by Gesture Recognition}
%TODO explain one-dollar and three-dollar
%TODO explain 3-dimensional GSS
%TODO explain gesture recognition indication (double tap)
The most casual form of interaction with the bracelet is by drawing gestures in the air to trigger basic operations, e.g. switching a light source on or off. These gestures are recorded by the bracelet's accelerometer and processed using the ``3\$ Gesture Recognizer'' \cite{Kratz2010}, an extension of the popular ``1\$ Recognizer'' by Wobbrock et. al \cite{Wobbrock2007}. The algorithm is explained in detail in the following paragraphs.

After a gesture is recorded, it is resampled to a fixed number of points. If the gesture was drawn quickly, it would have less samples and thus less points compared to a slowly drawn gesture. In order to be able to compare these two gestures, a resampling is performed before further processing. The length of the gesture path $M$ is calculated and an increment size $I$ derived by dividing $M$ by $(N/1)$, where $N$ is the desired number of samples. The gesture path is stepped trough and after each distance $I$, a new point is inserted using linear interpolation (cf. listing \ref{lst:resample}). %TODOwe have fixed # of samples. is this necessary?

\begin{lstlisting}[label=lst:resample,language=python,frame=lt,caption=Resampling of a points path into N evenly spaced points]
def resample(points, N):
	I = path_length(points) / (N - 1)
	D = 0
	newpoints = points[:1]
	for i in range(1, len(points)):
		dist = distance(points[i-1], points[i])
		if(D + dist) > I:
			q = points[i-1] + ((I - D)/dist) * (points[i] - points[i-1])
			newpoints.append(q)
			D = distance(q, points[i])
		else:
			D = D + dist
	if(D + dist) > increment: #append last point manually
		q = newpoints[-1] + ((increment - D) / dist) * (points[i] - newpoints[-1])
		newpoints.append(q)
	return newpoints
\end{lstlisting}

In the next step, the gesture is prepared for matching against the templates by rotating it to a specific position. The so called \textit{indicative angle} $\theta$ between the gesture's centroid $C$ and its first point is calculated using the normalized scalar product of the centroid and the first point's position vectors. Afterwards the gesture is rotated so that $\theta$ is at $0\degree$. Wobbrock et. al. do this by using the inverse tangens function, however this is not possible in 3D space. Hence, \textit{Rodrigues' rotation formula} (named after French mathematician Olinde Rodrigues) was implemented instead. This efficient algorithm for rotating a vector in $\mathbb{R}^3$ takes the rotation angle $\theta$ as well as the axis unit vector $k$ as input, and calculates the following formula:

\begin{center}
\(
v_{rot} = v \cos\theta + (k \times v)\sin\theta + k (k \cdot v) (1 - \cos\theta).
\) \cite{koks2006}
\end{center}

After applying this function on the gesture, a good starting point is created for the actual recognizing part of the algorithm. Listing \ref{lst:rotate2zero} illustrates the rotation procedure.

\begin{lstlisting}[label=lst:rotate2zero,language=python,frame=lt,caption=Rotation of points so that their indicative angle is at $0 \degree$]
def rotate_to_zero(points):
	c = centroid(points)
	theta = acos(points[0] * c / (|points[0]| * |c|)
	newpoints = rotate_by(points, -theta)
	return newpoints
\end{lstlisting}

In order to harmonize gestures of different sizes, the points are scaled to a reference cube with edge length of $100$ units and translated so that the respective centroid $C$ is on the origin (cf. listing \ref{lst:scale+translate}).

\begin{lstlisting}[label=lst:scale+translate,language=python,frame=lt,caption=Scaling to reference cube and translation to origin]
def scale_and_translate(points, size): # size=100
	B = Bounding_Box(points)
	newpoints = []
	for p in points:
		q = Point()
		q.x = (p.x * (size / B.width)) - c.x
		q.y = (p.y * (size / B.height)) - c.y
		q.z = (p.z * (size / B.depth)) - c.z
		newpoints.append(q)
	return newpoints
\end{lstlisting}

After these harmonization and preparation steps, the gestures are matched against the prerecorded templates (cf. listing \ref{lst:recognize}). The aforementioned steps are applied to templates as well as to newly recorded gestures, the following algorithms only apply to unrecognized gestures.

\begin{lstlisting}[label=lst:recognize,language=python,frame=lt,caption=Matching candidate gesture against every template]
def recognize(points, templates, rescale_size):
	theta_min = -180
	theta_max = 180
	theta_delta = 2

	best = float("inf")
	for t in templates:
		dist = distance_at_best_angle(points, t, theta_min, theta_max, theta_delta)
		if dist < best:
			best = dist
			t_best = t
	return t_best
\end{lstlisting}

The candidate is compared to all stored templates using the average \ac{MSE} as a scoring metric. The optimal position between the candidate gesture and a stored template is possibly offset by a certain rotation, so the optimal combination of angles between the two gestures needs to be determined. Since rotations are costly in terms od computation time, the candidate gesture should be aligned to the template in as few tries as possible. Hence, as proposed by \cite{Kratz2010}, a \ac{GSS} is used to find the optimal angles $\alpha$, $\beta$, and $\gamma$ for rotation around the three axis of the coordinate system.

The \ac{GSS} algorithm was invented by US-American statistician Jack Kiefer in 1953  and is conceptualized to find the minimum value $x^*$ of a unimodal function $f(x)$ in a given interval $[a,b]$ \cite{Kiefer1953}. Two function points $f(x_1)$ and $f(x_2)$ with 
\[ x_1=a+(1-\phi)(b-a) \\
x_2=a+\phi(b-a) \]
are calculated and compared, where $\phi=2/(1+\sqrt{5})$ denotes the Golden Section constant. If $f(x_1) < f(x_2)$, the interval for the next iteration becomes $[a,x_1]$ and the new test points are calculated as described above with respect to the new interval borders. Note that the value $f(x_1)$ can be reused \cite{chang2009n}. If $f(x_1) > f(x_2)$, the interval for the next iteration step changes to $[x_2, b]$ respectively. If the function points differ not more than a given $x_\Delta$, the current minimum will be accepted as the algorithm's result.

In the context of the implemented gesture recognizer, the function to be minimized is the distance between a gesture candidate and a certain template at the best angle. Since the recognizer works with three-dimensional data, there is not a single angle for rotation, but one fir each coordinate axis. Hence, the \ac{GSS} needs to be adapted for three dimensions.

As listing \ref{lst:daba} illustrates, the three-dimensional approach is similar to the one-dimensional algorithm. Each of the variables $\alpha$, $ \beta $, $ \gamma $ has its own search interval and a pair of calculated values. This leads to eight function points to be evaluated in each iteration, denoted in the code by variables \texttt{f1} to \texttt{f8}. The minimum of these values is calculated and the intervals adjusted accordingly. Note that in every case one function value from the previous step can be carried over, for example when \texttt{f1} is the minimum, its value is used for the new \texttt{f8}.

\begin{lstlisting}[label=lst:daba,language=python,frame=lt,caption=Three-dimensional Golden Section Search for finding the best angle between a candidate gesture and a template]
def distance_at_best_angle(p, t, theta_min, theta_max, theta_delta):
	phi = 0.5 * (-1 + math.sqrt(5))
	a_min = theta_min
	a_max = theta_max
	b_min = theta_min
	b_max = theta_max
	g_min = theta_min
	g_max = theta_max

	x1 = phi * a_min + (1 - phi) * a_max
	x2 = (1 - phi) * a_min + phi * a_max
	y1 = phi * b_min + (1 - phi) * b_max
	y2 = (1 - phi) * b_min + phi * b_max
	z1 = phi * g_min + (1 - phi) * g_max
	z2 = (1 - phi) * g_min + phi * g_max	

	f1 = distance_at_angle(p, t, x1, y1, z1)
	f2 = distance_at_angle(p, t, x1, y1, z2)
	f3 = distance_at_angle(p, t, x1, y2, z1)
	f4 = distance_at_angle(p, t, x1, y2, z2)
	f5 = distance_at_angle(p, t, x2, y1, z1)
	f6 = distance_at_angle(p, t, x2, y1, z2)
	f7 = distance_at_angle(p, t, x2, y2, z1)
	f8 = distance_at_angle(p, t, x2, y2, z2)

	while (|a_max - a_min| > theta_delta) or (|b_max - b_min| > theta_delta) or (|g_max - g_min| > theta_delta):	
		min_f = min(f1, f2, f3, f4, f5, f6, f7, f8)
		if min_f == f1: #x1, y1, z1
			a_max = x2
			x2 = x1
			x1 = phi * a_min + (1 - phi) * a_max
			b_max = y2
			y2 = y1
			y1 = phi * b_min + (1 - phi) * b_max
			g_max = z2
			z2 = z1
			z1 = phi * g_min + (1 - phi) * g_max
			f8 = f1
			f1 = distance_at_angle(p, t, x1, y1, z1)
			...
			f7 = distance_at_angle(p, t, x2, y2, z1)
		elif min_f == f2: #x1, y1, z2
			...
		else: #x2, y2, z2
			...
	return min(f1, f2, f3, f4, f5, f6, f7, f8)
\end{lstlisting}

To determine the distance between a gesture and a certain template at given angles $ \alpha $, $ \beta $, and $ \gamma $, the gesture is rotated using the Rodrigues rotation formula discussed in the context of the indicative angle above. The rotation formula needs an axis and an angle for executing the rotation, but the \ac{GSS} works with three angles. Therefore, the rotation matrix formed by the angles needs to be transformed into a Euler axis/angle pair for use in the implemented rotation formula. The authors of \cite{Kratz2010} state the following rotation matrix in their reference implementation \cite{repo:3dollar}:

\[
A = 
\begin{pmatrix}
\cos{\alpha}\cos{\beta} & \cos{\alpha}\sin{\beta}\sin{\gamma} - \sin{\alpha}\cos{\gamma} & \cos{\alpha}\sin{\beta}\cos{\gamma} + \sin{\alpha}\sin{\gamma} \\
\sin{\alpha}\cos{\beta} & \sin{\alpha}\sin{\beta}\sin{\gamma} + \cos{\alpha}\cos{\gamma} & \sin{\alpha}\sin{\beta}\cos{\gamma} - \cos{\alpha}\sin{\gamma} \\
-\sin{\beta} & \cos{\beta}\sin{\gamma} & \cos{\beta}\cos{\gamma}
\end{pmatrix}
\]

The Euler rotation angle $ \theta $ is composed by the matrix's diagonal elements.
\begin{eqnarray*}
\theta & = & \arccos(\frac{1}{2}(A_{11} + A_{22} + A_{33} - 1)) \\
       & = & \arccos(\frac{1}{2}(\cos\alpha\cos\beta+\sin\alpha\sin\beta\sin\gamma+\cos\alpha\cos\gamma+\cos\beta\cos\gamma-1))
\end{eqnarray*}
The rotation axis vector $ e $ can be derived from $ A $ and $ \theta $ as follows:
\begin{eqnarray*}
e_1 & = &  \frac{A_{32} - A_{23}}{2\sin\theta} \\
    & = & \frac{\sin\alpha\sin\beta\cos\gamma-\cos\alpha\sin\gamma-\cos\beta\sin\gamma}{2\sin\theta} \\
e_2 & = & \frac{A_{13} - A_{31}}{2\sin\theta} \\
    & = & \frac{-\sin\beta-\cos\alpha\sin\beta\cos\gamma-\sin\alpha\sin\gamma}{2\sin\theta} \\
e_3 & = & \frac{A_{21} - A_{12}}{2\sin\theta} \\
    & = & \frac{\cos\alpha\sin\beta\sin\gamma-\sin\alpha\cos\gamma-\sin\alpha\cos\beta}{2\sin\theta}
\end{eqnarray*}

An so, the angles determined in algorithm \ref{lst:daba} lead to an axis/angle rotation which is performed on the candidate gesture.

Once the best template for a candidate gesture is found, the corresponding index in the stored template list is returned and the recognized gesture triggers certain changes in the light source's color or brightness. Note that this gesture recognition algorithm is only used for recognition of the preset gestures formulated in section XX. %TODO


\section{Presets and Configuration}

Gesture recording and recognition is triggered by a gentle double tap on the bracelet. This small but focused activation reduces unwanted triggering of the gesture recognition process, e.g. while gesturing heavily, and thus reducing false positives. The tap detection functionality is a built-in feature of the bracelet's accelerometer, configuration parameters for this process are listed in table \ref{tab:tapconf}.

In order to be recognized as a tap interaction, the initial impulse needs to be at least XX g in intensity. When calibrated like this, jerky movements like suddenly raising the hand at a high speed are correctly not recognized as a tap. However since the threshold is that high, the activation tap needs to be executed directly on the hardware which is located on the inner wrist.

The PULSE\_TMLT register configures the maximum time interval between the  impulse exceeding the threshold on the Z axis and falling back under said threshold. If the mentioned interval lasts at most $6.25 ms$, the interaction is considered as a tap.

After a tap is detected, all impulses in the following $25ms$ are ignored by the detection mechanism. This prevents bouncing effects and detecting multiple taps in a singe tap movement.

The MMA8652FC accelerometer is able to distinguish between single and double taps. The last configuration register listed in table \ref{tab:tapconf} is a parameter for double tap detection. It specifies the maximum time interval between two double taps and is set to $500 ms$, the same time interval as the Windows default between two mouse clicks of a double click \cite{doubleclick}.

\begin{table}
	\myfloatalign
	\begin{tabularx}{\textwidth}{lll} \toprule
		\tableheadline{Register Name} & \tableheadline{Parameter} & \tableheadline{Value}\\ 
		\midrule
		PULSE\_THSZ & Tap Detection Threshold & $100 g$\\ %TODO on +/-8g scale @.063g/LSB
		PULSE\_TMLT & Interval between Start and End Pulse & $6.25 ms$\\
		PULSE\_LTCY & Ignore Interval after Detection & $25 ms$\\
		PULSE\_WIND & Maximum Double Tap Interval & $500ms$ \\
		\bottomrule
	\end{tabularx}
	\caption[Tap detection configuration]{Single- and double tap detection configuration for the MMA8652FC digital accelerometer}  \label{tab:tapconf}
\end{table}