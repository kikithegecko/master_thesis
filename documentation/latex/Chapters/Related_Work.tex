\chapter{Related Work}
Casual interaction has been explored by other researchers.

Pohl and Murray-Smith have researched casual interaction with everyday electronics like smartphones. They coined the term ``casual interaction'' in contrast to focused interaction. They described that in many situations in daily life, the user would not receive periodic feedback from the interacting device or not interact with it in a constant, uninterrupted fashion due to physical, social or mental reasons like wearing gloves in the winter or being exhausted after a day of work. Instead of giving up control over the device in such situations, the user should be able to control the level of focus to which they would like to engage \cite{Pohl2013}. The underlying distinction between foreground and background interaction in context of technology has been introduced by Buxton in 1995, where he described improvements on telemetry systems. As a background interaction between humans, the office cameras of the system's participants take periodic stills and check if all members of a scheduled video conference are at their desks. Once all participants are available, the software actively asks to set up a conference and thus switches to foreground interaction. This relieves the user of periodically interrupting her work and checking if all participants are available \cite{Buxton1995}.

Interaction with intelligent lighting in public space as well as in the arts has been researched by Seitinger et. al. The \textit{Urban Pixels} project encouraged users to interact with flexible \ac{LED} light sources which pose an alternative to current urban display systems \cite{Seitinger2009}, while the \textit{Light Bodies} project focused on people's interactions with and reactions to portable light sources that respond to sound and vibration \cite{Seitinger2010}. In both cases, the users enjoyed manipulating the lights and were eager to interact with them, often in a playful fashion.

Wearable electronic devices have been very present in the recent years. Activity trackers like the Fitbit bracelets \cite{fitbit} record the wearer's movement data and calculate statistics on daily activity, e.g. the number of stairs the user has climbed or the distance she rode by bicycle. Activity trackers thus aim to help the wearers in getting to know their daily habits and reaching goals in personal activity. Smart watches connect to mobile phones using Bluetoothm, they display various notifications along with the current time, allow for music playback control or aid in navigational tasks. They are either universal like the Pebble \cite{pebble} or bound to a certain phone like Samsung's Galaxy Gear \cite{galaxygear}.

More recently, attempts to add ``social'' features to wrist-worn smart devices have been made. The \textit{iBand} concept by Kanis et. al attempts to augment virtual social networks by merging their information about a person into real-world interactions with people. The device  triggers an automatic exchange of contact information if two users perform a handshake. Furthermore, the user can personalize a displayed logo and ``collect'' other users' emblems by exchanging contact information with them \cite{Kanis2005}. As a commercial product, the Razer Nabu smart band can exchange contact information on a handshake and detect other nearby users. The product merges these ``social'' functions with an activity tracker as described above \cite{razernabu}.

Controlling devices in the smart home is mostly realized by trigger-action programming as discussed by Ur et. al. On the IFTTT platform, users can create simple ``if, then'' rules for linking a broad range of actions between different software or devices, allowing only one trigger and only one action per rule. Ur et. al. discovered that a model like this for expressing rules would suffice to formulate behavior for the smart home if more flexibility in formulating the rules was allowed \cite{Ur2014}. A more complex approach to controlling the smart home is the use of middleware like \textit{Android Wear} for customized applications and use of an app store, allowing to pick the right applications for the smart home components available \cite{androidwear}.

Regarding new shapes and configurations of wrist-worn wearables, research has come up with various concepts. The \textit{Facet} bracelet features multiple displays and allows users to rearrange apps between them or extend a single app over multiple segments \cite{Lyons2012}. Kim et. al. implement remote control of other devices by performing hand gestures which are recognized by the \textit{Gesture Watch}, a set of proximity sensors worn on the wrist \cite{Kim2007}. The \textit{Snaplet} flexible touch display consists of an electrophoretic display augmented by sensors that can detect the current shape of the device. Bending the display causes \textit{Snaplet} to change its function (e.g. phone, media player) or allows the user to naviagte through menus \cite{Tarun2011}. Ni and Baudisch even suggest reducing the size of the wearable device to a tiny worn sensor that uses the surrounding skin on the arm or any other body part as an input canvas for touch and gesture input, so that the device blends in with the user's clothing or skin. Despite their small size, users are still able to issue various commands and enter text by using a graffiti gesture alphabet \cite{Ni2009}.

For gesture recognition, two major algorithms that originate from the field of machine learning are very present in research projects. The Gesture and Activity Recognition Toolkit \textit{GART} has been developed by Lyons et. al. and is based on Hidden Markov Models, a statistical model also used in fields like speech or writing recognition \cite{Lyons2012}. \textit{GART} has been used by the aforementioned \textit{Gesture Watch} \cite{Kim2007} and the \textit{AirTouch} device, an around-device gesture recognition device that gives tactical feedback in form of vibrations \cite{Lee2011}. Several other gesture recognizing wearables implement Dynamic Time Warping, another algorithm from the field of machine learning that matches temporal sequences that very in speed \cite{Ashbrook2010} \cite{Liu2009}. In contrast to those computation-heavy and complex algorithms, Wobbrock et. al. have developed a simpler, geometry-based gesture recognizer for touch screen interaction that compares shapes against templates by scaling, translating and rotating them while measuring the distance to find a minimum \cite{Wobbrock2007}. The algorithm was extended to 3D accelerometer data by Kratz and Rohs \cite{Kratz2010}. The recognizer was designed as easy to implement and functional even on embedded devices with limited computation capabilities.